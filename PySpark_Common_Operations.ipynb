{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case : Data Profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+----------+-------------------+\n",
      "|   a|   b|      c|         d|                  e|\n",
      "+----+----+-------+----------+-------------------+\n",
      "|   1| 2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|   2| 3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|   4| 5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|   4| 5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|   4| 5.0|string3|2000-05-01|2000-01-03 12:00:00|\n",
      "|null| 5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|   6|null|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+----+----+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)), # Totally duplicate with previous line \n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 5, 1), e=datetime(2000, 1, 3, 12, 0)), # PK duplicate\n",
    "    Row(a=None, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)), # PK contains Null\n",
    "    Row(a=6, b=None, c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)) # PK contains Null\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK = ['a','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+----------+-------------------+----------------+\n",
      "|   a|   b|      c|         d|                  e|            desc|\n",
      "+----+----+-------+----------+-------------------+----------------+\n",
      "|null| 5.0|string3|2000-03-01|2000-01-03 12:00:00|PK Contains Null|\n",
      "|   6|null|string3|2000-03-01|2000-01-03 12:00:00|PK Contains Null|\n",
      "+----+----+-------+----------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PK null check\n",
    "from pyspark.sql.functions import lit\n",
    "null_check_str = ' is null or '.join(PK) + ' is null'\n",
    "df_null = df.filter(null_check_str)\n",
    "df_null = df_null.withColumn('desc',lit('PK Contains Null'))\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is not null and b is not null and c is not null\n",
      "+---+---+-------+----------+-------------------+-------------+\n",
      "|  a|  b|      c|         d|                  e|         desc|\n",
      "+---+---+-------+----------+-------------------+-------------+\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|Duplicate row|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|Duplicate row|\n",
      "|  4|5.0|string3|2000-05-01|2000-01-03 12:00:00|Duplicate row|\n",
      "+---+---+-------+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "not_null_check_str = ' is not null and '.join(PK)+' is not null'\n",
    "\n",
    "df = df.where(not_null_check_str)\n",
    "df_dups = df.groupby(PK).count().filter('count>1').select(PK).withColumn('desc',lit('Duplicate row'))\n",
    "df.join(df_dups,on = PK,how = 'left').filter('desc is not null').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|   a|   b|      c|\n",
      "+----+----+-------+\n",
      "|   1| 2.0|string1|\n",
      "|   2| 3.0|string2|\n",
      "|null| 5.0|string3|\n",
      "|   6|null|string3|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df_dups,on=PK,how='left').where('count is null').select(PK).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pk_null_check(df,cols = []):\n",
    "    from pyspark.sql.functions import lit\n",
    "    \n",
    "    null_check_str = ' is null or '.join(cols) + ' is null'\n",
    "    df_null = df.filter(null_check_str)\n",
    "    df_null = df_null.withColumn('desc',lit('PK Contains Null'))\n",
    "    return df_null\n",
    "\n",
    "def pk_dups_check(df,cols = []):\n",
    "    not_null_check_str = ' is not null and '.join(cols)+' is not null'\n",
    "    df_dups = df.where(not_null_check_str).groupby(PK).count().filter('count>1').select(PK).withColumn('desc',lit('Duplicate row'))\n",
    "    \n",
    "    return df.join(df_dups,on = PK,how = 'sleft').filter('desc is not null')\n",
    "\n",
    "\n",
    "def profiling_data(df,PK = []):\n",
    "    df_null = pk_null_check(df,PK)\n",
    "    df_dups = pk_dups_check(df,PK)\n",
    "    \n",
    "    df_reject = df_null.union(df_dups)\n",
    "    \n",
    "    not_null_check_str = ' is not null and '.join(PK)+' is not null'\n",
    "    df_clean = df.filter(not_null_check_str).join(df_dups.groupby(PK).count(),on=PK,how='left').filter('count is null').select(df.columns)\n",
    "    return df_clean,df_reject\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean,reject = profiling_data(df,PK)\n",
    "clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+----------+-------------------+----------------+\n",
      "|   a|   b|      c|         d|                  e|            desc|\n",
      "+----+----+-------+----------+-------------------+----------------+\n",
      "|null| 5.0|string3|2000-03-01|2000-01-03 12:00:00|PK Contains Null|\n",
      "|   6|null|string3|2000-03-01|2000-01-03 12:00:00|PK Contains Null|\n",
      "|   4| 5.0|string3|2000-03-01|2000-01-03 12:00:00|   Duplicate row|\n",
      "|   4| 5.0|string3|2000-03-01|2000-01-03 12:00:00|   Duplicate row|\n",
      "|   4| 5.0|string3|2000-05-01|2000-01-03 12:00:00|   Duplicate row|\n",
      "+----+----+-------+----------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reject.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case : Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case : Row to Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
